# -*- coding: utf-8 -*-
"""CV_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RMnRvw1ZcGv-xOPABo04rb7FAB8u3Cce
"""

!pip install opencv-python==3.4.2.16
!pip install opencv-contrib-python==3.4.2.16

import os
import numpy as np
import cv2 as cv
from PIL import Image
from pprint import pprint
import glob
from google.colab.patches import cv2_imshow
from sklearn.cluster import MiniBatchKMeans, KMeans
from sklearn.preprocessing import scale
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.mixture import GaussianMixture
from sklearn.neighbors import NearestNeighbors
from collections import defaultdict
from sklearn.ensemble import RandomForestClassifier, RandomTreesEmbedding
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.feature_selection import SelectFromModel
from sklearn.metrics import plot_confusion_matrix, confusion_matrix
from sklearn.linear_model import SGDClassifier
from sklearn.kernel_approximation import AdditiveChi2Sampler
from sklearn.svm import SVC

from google.colab import drive
drive.mount('/content/drive/')
os.chdir("/content/drive/My Drive/Caltech_101/101_ObjectCategories/")

"""# Data ingestion"""

# get the image data from the google drive 
# the 10 labels are predifined

labels = ["tick", "trilobite", "umbrella", "watch", "water_lilly", "wheelchair", "wild_cat", "windsor_chair", "wrench", "yin_yang"]
images = []
for label in labels:
  image_list = list(map(Image.open, glob.glob('{l}/*.jpg'.format(l=label))))
  images.append(image_list)
images = np.array(images)

images[2][0]

# split the dat ain training and test with 10 classes in total of 15 images each
x_train, x_test = [], []
n_train, n_test = 15, 15
for l in images:
  x_train.append((l[:n_train]))
  x_test.append((l[-n_test:]))

"""# SIFT"""

# img = cv.imread(x_train[2][0], 0)
img = cv.cvtColor(np.array(x_train[2][0]), cv.COLOR_RGB2GRAY)

sift = cv.xfeatures2d.SIFT_create()
kp, des = sift.detectAndCompute(img, None)

kp_img = cv.drawKeypoints(img, kp, img, flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

cv2_imshow(kp_img)

# create ndarray of the sifts of each image. 
# have one array where is distributed per image - single
# have another array where all the sifts are in, independendt of their class - joint

def sift_data(x):
  kp_train_single = []
  sift = cv.xfeatures2d.SIFT_create()
  for label_img in x:
    kp_img_category = []
    for l in label_img:
      img = cv.cvtColor(np.array(l), cv.COLOR_RGB2GRAY) if l.mode == 'RGB' else np.array(l)
      kp, des = sift.detectAndCompute(img, None)
      kp_img_category.append(des)
    kp_train_single.append(kp_img_category)
  kp_train_single = np.array(kp_train_single)
  return np.vstack(np.ndarray.flatten(kp_train_single))

kp_train_joint = sift_data(x_train)
kp_train_joint.shape

# matching representation

umb1 = x_train[2][0]
umb2 = x_train[2][1]

umb1 = cv.cvtColor(np.array(umb1), cv.COLOR_RGB2GRAY) if umb1.mode == 'RGB' else np.array(umb1)
umb2 = cv.cvtColor(np.array(umb2), cv.COLOR_RGB2GRAY) if umb2.mode == 'RGB' else np.array(umb2)

sift = cv.xfeatures2d.SIFT_create()

kp1, des1 = sift.detectAndCompute(umb1, None)
kp2, des2 = sift.detectAndCompute(umb2, None)

# feature matching
bf = cv.BFMatcher(cv.NORM_L1, crossCheck=True)
matches = bf.match(des1, des2)
matches = sorted(matches, key=lambda x:x.distance)

match_img = cv.drawMatches(umb1, kp1, umb2, kp2, matches[:50], umb2, flags=2)
cv2_imshow(match_img)

"""# Visual vocabulary construction (Codewords)"""

# standarise data 
data = scale(kp_train_joint, axis=1)  # standarise each of the 65k samples
data.shape

# pca
# pca = PCA(0.99, whiten=True)    # preserve 99% of the variance in the data
# data_pca = pca.fit_transform(data)
# data_pca.shape

"""## Finding optimum cluster number K with AIC & BIC"""

# test different cluster numbers k
k_models = dict()
for k in range(100, 1000, 50):
  kmeans = MiniBatchKMeans(init ='k-means++', n_clusters = k, 
                           batch_size = 1000, n_init = 10, 
                           random_state=1, verbose = 0).fit(data) 
  k_models[k] = (kmeans.inertia_, kmeans)

# draw cluster vs. inertia to observe elbow method
inertias = [inertia for inertia,model in k_models.values()]
k_clusters = [k for k in k_models.keys()]

plt.plot(k_clusters, inertias, 'bo')
plt.ylabel('inertia')
plt.xlabel('cluster numbers')
plt.show()

# gaussian model to obtain the aic
n_components = np.arange(50, 500, 50)
gmm = {k:GaussianMixture(k, covariance_type='diag', random_state=0, verbose=2).fit(data) for k in n_components}

for k in range(600, 700, 50):
  gmm[k] = GaussianMixture(k, covariance_type='diag', random_state=0, verbose=2).fit(data)

aics = [model.aic(data) for model in gmm.values()]
bics = [model.bic(data) for model in gmm.values()]

plt.plot(list(gmm.keys()), aics, 'bo', label="aic")
plt.plot(list(gmm.keys()), bics, 'r+', label="bic")
plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left',
           ncol=2, mode="expand", borderaxespad=0.)
plt.show()

"""## K-Means"""

# perform kmeans with k clusters and pca data
k = 250

kmeans = KMeans(init ='k-means++', n_clusters = k, n_init = 10, 
                random_state=0, verbose = 0).fit(data)

codewords = kmeans.cluster_centers_
codewords.shape

"""## Random Trees Embedding"""

rtree = RandomTreesEmbedding(n_estimators=1000, max_depth=70, 
                             min_samples_leaf=1, min_samples_split=2,
                             verbose=1, random_state=0)

rtree.fit(data)

# For each datapoint x in X and for each tree in the forest, 
# return the index of the leaf x ends up in.
leafs = rtree.apply(data)

leafs.shape

"""# Histogram of visual words"""

# note how many SIFTs are per image knowing there are 150 images

def count_sifts_per_image(x):
  sift = cv.xfeatures2d.SIFT_create()
  n_sift = []
  for label_img in x:
    for l in label_img:
      img = cv.cvtColor(np.array(l), cv.COLOR_RGB2GRAY) if l.mode == 'RGB' else np.array(l)
      kp, des = sift.detectAndCompute(img, None)
      n_sift.append(des.shape[0])
  return n_sift

train_n_sift = count_sifts_per_image(x_train)
len(train_n_sift)

# vw = visual words
def visualWords(data):
  vw = []
  p1, p2 = 0, 0
  for n in train_n_sift:
    p2 += n 
    vw.append(data[p1:p2])
    p1 = p2
  return np.array(vw)

# KMeans codebook
vw = visualWords(data)

# Nearest Neighbour

# tree data
nn = NearestNeighbors(n_neighbors=1, algorithm='brute').fit(trees_data)

# balltree
# nn = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(trees_data)

# kd_tree
# nn = NearestNeighbors(n_neighbors=1, algorithm='kd_tree').fit(codewords)

# brute
# nn = NearestNeighbors(n_neighbors=1, algorithm='kd_tree').fit(codewords)

# build histogram of images
# the dictionary key corresponds to the index of the codeword
# the dictionary value corresponds to the frequency of that codeword 

train_hist = []
for img in vw:
  histogram = defaultdict(int)
  for visual_word in img:
    dist, index = nn.kneighbors([visual_word])
    histogram[index[0][0]] += 1
  train_hist.append(histogram)

# draw the image with its codeword representation
class_number = 2  # umbrella
img_number = class_number*15 + 0

labels = np.arange(codewords.shape[0])
umb1 = [0 for _ in range(codewords.shape[0])]
umb2 = [0 for _ in range(codewords.shape[0])]
for u1,u2 in zip(train_hist[img_number].items(), train_hist[img_number+1].items()):
  k1, f1 = u1
  k2, f2 = u2
  umb1[k1], umb2[k2] = f1, f2  

width = 0.5
fig, ax = plt.subplots()
rects1 = ax.bar(labels-width/2, umb1, width, label='Umbrella 1')
rects2 = ax.bar(labels+width/2, umb2, width, label='Umbrella 2')

ax.set_ylabel('Frequency')
ax.set_title('Codewords comparison between 2 different umbrellas')
ax.set_xlabel('Codeword K')
ax.legend()

# fig.tight_layout()
plt.show()

"""# Random Forest (RF)"""

# RF codebook
rf_train_features = visualWords(leafs)
rf_train_features.shape

labels = ["tick", "trilobite", "umbrella", "watch", "water_lilly", "wheelchair", "wild_cat", "windsor_chair", "wrench", "yin_yang"]
y_train = []
for label in labels:
  y_train.append([label]*n_train)
y = np.array(y_train).flatten()
y.shape

# create data array for training RF

def data_for_RF(histogram):
  data = np.zeros((150,k), dtype=int)
  for idx,img in enumerate(histogram):
    for key,val in img.items():
      data[idx][key] = val
  return data

train_features = data_for_RF(train_hist)
train_features.shape

"""### Cross Validation (CV)"""

"""
n_estimators: number of trees to be used in the forest
max_features: max number of features to consider for splitting a node
max_depth: max number of levels in each decision tree
min_samples_split: min number of data points placed in a node before the node is split
min_samples_leaf: min number of data points allowed in a leaf node
bootstrap: method for sampling data points (with or without replacement)
"""

{'bootstrap': True,
 'max_depth': 70,
 'max_features': 'auto',
 'min_samples_leaf': 1,
 'min_samples_split': 2,
 'n_estimators': 1000}

rf = RandomForestClassifier(n_estimators=1000, criterion='gini', 
                            max_depth=70, min_samples_split=2, 
                            min_samples_leaf=1, max_features='auto',  
                            bootstrap=True, random_state=0, verbose=1)

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]
# Number of features to consider at every split
max_features = ['sqrt', 'log2', 'auto']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 4]
# Method of selecting samples for training each tree
bootstrap = [True, False]

# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}

pprint(random_grid)

# Use the random grid to search for best hyperparameters

# First create the base model to tune
rf = RandomForestClassifier()
# Random search of parameters, using 3 fold cross validation, 
# search across 100 different combinations, and use all available cores
"""
n_iter: controls of different combinations to try
cv: number of folds to use for cross validation 
More iterations will cover a wider search space and 
more cv folds reduces the chances of overfitting, 
but raising each will increase the run time. 
Machine learning is a field of trade-offs, and performance 
vs time is one of the most fundamental.
"""

rf_random = RandomizedSearchCV(estimator = rf, 
                               param_distributions = random_grid, 
                               n_iter = 100, cv = 3, verbose=2, 
                               random_state=0, n_jobs = -1)
# Fit the random search model
rf_random.fit(train_features, y)

rf_random.best_params_

"""### Grid Search with CV"""

# Create the parameter grid based on the results of random search 
param_grid = {
    'bootstrap': [True],
    'max_depth': [80, 90, 100, 110],
    'max_features': [2, 3],
    'min_samples_leaf': [1, 2, 3],
    'min_samples_split': [1, 2, 3],
    'n_estimators': [600, 800, 1200, 1400]
}
# Create a based model
rf = RandomForestClassifier()
# Instantiate the grid search model
grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, 
                           cv = 3, n_jobs = -1, verbose = 2)

# Fit the grid search to the data
grid_search.fit(train_features, y)

grid_search.best_params_

"""### Create test features"""

# create test_features
test_features = sift_data(x_test)
test_features = scale(test_features, axis=1)  # standarise each of the 65k samples
pca = PCA(0.99, whiten=True)    # pca: preserve 99% of the variance in the data
# test_features = pca.fit_transform(test_features)
test_features.shape

# count test sifts
test_n_sift = count_sifts_per_image(x_test)
# vw = visual words
vw_test = []
p1, p2 = 0, 0
for n in test_n_sift:
  p2 += n 
  vw_test.append(test_features[p1:p2])
  p1 = p2
vw_test = np.array(vw_test)
vw_test.shape

# visual words to histogram representation using training codebook
test_hist = []
for img in vw_test:
  histogram = defaultdict(int)
  for visual_word in img:
    dist, index = nn.kneighbors([visual_word])
    histogram[index[0][0]] += 1
  test_hist.append(histogram)

test_features = data_for_RF(test_hist)
test_features.shape

"""### Evaluate Random Search"""

model = rf.fit(rf_train_features, y)

def evaluate(model, test_features, test_labels):
    predictions = model.predict(test_features)
    correct = np.sum(predictions == test_labels)
    accuracy = correct/len(test_labels)*100
    print('Model Performance')
    print('Accuracy = {:0.2f}%.'.format(accuracy))
    
    return accuracy

"""## Results"""

# sgd of chi squared kernel, 250 codewords
evaluate(model, test_features_transformed, y)

# cross validation model without pca and 250 codewords and nn=brute-force
evaluate(rf_random.best_estimator_, test_features, y)

# cross validation model without pca and 250 codewords and nn=kd_trees
evaluate(rf_random.best_estimator_, test_features, y)

# grid search cross validation without pca and 250 codewords
evaluate(grid_search.best_estimator_, test_features, y)

# cross validation model without pca and 250 codewords
evaluate(rf_random.best_estimator_, test_features, y)

# default model without pca and 400 codewords 
# evaluate(model, test_features, y)

# default model without pca and 100 codewords 
# evaluate(model, test_features, y)

# default model without pca and 600 codewords 
# evaluate(model, test_features, y)

# default model without pca and 250 codewords
# evaluate(model, test_features, y)

# default model with pca and 250 codewords 
# evaluate(model, test_features, y)

# cross validation model with pca and 250 codewords
# evaluate(model, test_features, y)

"""## Confussion Matrix"""

y_est = rf_random.best_estimator_.predict(test_features)
y_est

label = ["tick", "trilobite", "umbrella", "watch", "water_lilly", "wheelchair", "wild_cat", "windsor_chair", "wrench", "yin_yang"]
y = np.array([[l]*15 for l in label]).flatten()

np.sum(labels==y_est)/150*100

cf = confusion_matrix(y, y_est, label)

import seaborn as sn
import pandas as pd
df_cm = pd.DataFrame(cf, index = [i for i in label],
                     columns = [i for i in label])
plt.figure(figsize = (10,7))
sn.heatmap(df_cm, annot=True)